---
layout: post
type: blog
title: Privacy Preserving Machine Learning 🔐
comments: true
mathjax: true
---

<p>Lately, I’ve been working on <a href="https://twitter.com/hashtag/PrivacyPreservingML">#PrivacyPreservingML</a> 🔐 I got looped in some projects after Apple launched <a href="https://developer.apple.com/documentation/apptrackingtransparency">AppTrackingTransparency (ATT)</a> framework, requiring iOS apps to ask permission to share users’ data w/ 3rd parties. This has triggered an industry-wide discussion on best practices to respect user privacy.</p>

<center>
  <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Lately, I’ve been working on <a href="https://twitter.com/hashtag/PrivacyPreservingML?src=hash&amp;ref_src=twsrc%5Etfw">#PrivacyPreservingML</a> 🔐<br><br>I got looped in some projects after Apple launched ATT, requiring iOS apps to ask permission to share users’ data w/ 3rd parties. <br><br>This has triggered an industry-wide discussion on best practices to respect user privacy. <a href="https://t.co/Ne1aF3p9F9">pic.twitter.com/Ne1aF3p9F9</a></p>&mdash; Jigyasa Grover  (@jigyasa_grover) <a href="https://twitter.com/jigyasa_grover/status/1546358981660069888?ref_src=twsrc%5Etfw">July 11, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

<p>ML models are as good as the data we feed them 🍣 In the online world, it's tempting to use data from a user's each &amp; every move by sharing info across companies to make the models smarter -- in the guise of <em>‘serving the users better’</em>. <a href="https://twitter.com/hashtag/DataCentricAI">#DataCentricAI</a></p>

<p>But the use of every data point w/o protecting it has led to cyber attacks, reverse engineering, and violations of sensitive data like personal conversations, financial transactions, medical history, etc. This is where <a href="https://twitter.com/hashtag/PrivacyPreservingML">#PrivacyPreservingML</a> comes into play!</p>

<hr>

<p>To put it simply:</p>

<p style="font-size:25px"><strong>Privacy-Preserving ML = Data Privacy + Model Privacy</strong></p>

<hr>

<p><strong>Data Privacy</strong> = Using authorized data + not exposing training data &amp; its source + protecting input &amp; output data. Check out this <a href="https://dl.acm.org/doi/10.5555/3361338.3361358">“secret sharer”</a> paper -- certain sequences (for eg. credit card numbers, SSN) can be unintentionally memorized!</p>

<p><strong>Model Privacy</strong> = Protect the model and its nuances (aka parameters) from the public eye. Apart from Intellectual Property loss, using it to reverse engineer inputs from the outputs can prove to be a dangerous problem in many sensitive situations.</p>

<p>There are many methods to ensure that the data cannot be stolen by a third party or how to ensure model privacy. Might come back to these <a href="https://twitter.com/hashtag/PrivacyPreservingML">#PrivacyPreservingML</a> techniques some other time, until then CIAO 👋🏻</p>

<p>On that note, if you are someone who is still getting started on <a href="https://twitter.com/hashtag/DataForML">#DataForML</a> and need a boost to your <a href="https://twitter.com/hashtag/MachineLearning">#MachineLearning</a> dataset building skills using <a href="https://twitter.com/hashtag/Python">#Python</a> and all things <a href="https://twitter.com/hashtag/OpenSource">#OpenSource</a>! Check out <a href="https://twitter.com/DataForML">@DataForML</a> on <a href="https://twitter.com/amazon">@amazon</a> 🛒</p>


<center>
  <iframe type="text/html" width="336" height="550" frameborder="0" allowfullscreen style="max-width:100%" src="https://read.amazon.com/kp/card?asin=B08RN47C5T&preview=inline&linkCode=kpe&ref_=cm_sw_r_kb_dp_SM5Z75YRJTFFDKF860QN&tag=mobile0a1329f-20" ></iframe>
 </center>
 
<hr>
