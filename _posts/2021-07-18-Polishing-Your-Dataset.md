---
layout: post
type: blog
title: 5 tips for polishing your Machine Learning dataset ğŸ§¼
comments: true
mathjax: true
---
<p>I have been professionally working as a Machine Learning Engineer since more than 2 years now and also, recently co-authored a book titled <strong><a rel="noreferrer noopener" href="https://www.amazon.com/dp/B08RN47C5T" target="_blank">â€œSculpting Data for ML: The first act of Machine Learningâ€</a></strong>. My past few experience have taught me that data does not get its due limelight in <a href="https://twitter.com/hashtag/MachineLearning">#MachineLearning</a> as compared to complex model architecture. Keeping up with 'more data beats clever algorithms, but better data beats more data', here are top 5 tips for polishing the dataset to effectively solve <a href="https://twitter.com/hashtag/ML">#ML</a> problems ğŸ¤–ğŸ‘‡ğŸ»</p>

</br>

<center>
 <blockquote class="twitter-tweet">
  <p lang="en" dir="ltr">Honestly, data does not get its due limelight in <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> as compared to complex model architecture. Keeping up with &#39;more data beats clever algorithms, but better data beats more data&#39;, here are top 5 tips for polishing the dataset to effectively solve <a href="https://twitter.com/hashtag/ML?src=hash&amp;ref_src=twsrc%5Etfw">#ML</a> problems ğŸ¤–ğŸ‘‡ğŸ»
  </p>&mdash; Sculpting Data for ML ğŸ“– (@DataForML)
  <a href="https://twitter.com/DataForML/status/1416975965209522186?ref_src=twsrc%5Etfw">July 19, 2021</a>
 </blockquote> 
 <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

</br>

<p><strong>DATA IS SEASONALâ˜ƒï¸ğŸğŸŒ§</strong> Every data point has its expiration date! With infinite data streams today, it is important to continuously perform data distribution checks to maintain the â€˜data is IIDâ€™ status, more so if you are training on-the-go.</p>

<p><strong>FIGHT THE BIASğŸ¤ºğŸ§¯ğŸ’¥ </strong>While data might represent the ultimate truth, the way a dataset is created might not. Any form of <a href="https://twitter.com/hashtag/DatasetBias">#DatasetBias</a> can limit generalization capabilities of even the most sophisticated <a href="https://twitter.com/hashtag/ML">#ML</a> algorithm. And thus, unintentionally lead to collective, disparate impact.</p>

<p><strong>MIX IT UP ğŸ¹ğŸ¸ğŸ¥ƒ</strong> Many times, simply not having enough data becomes a blocker. In such cases, it helps to identify related datasets and combine them using horizontal/vertical integration. If that is not possible, data augmentation and oversampling techniques may come in handy.</p>

<p><strong>PROBLEM OF PLENTY âœ‚ï¸ğŸ‘ğŸ»ğŸ¬</strong> More data def helps, but sometimes a huge dataset takes a toll on training time and computation resources. Cutting down on data points while still maintaining model performance by using techniques like importance sampling or stratified sampling helps.</p>

<p><strong>DATA ABOUT THE DATA ğŸ¤“ğŸ”¬ğŸ“ˆ</strong> Any dataset we use must have enough metadata. Sufficient amount of side-information helps in engineering more/better features so that our <a href="https://twitter.com/hashtag/ML">#ML</a> model can decipher patterns more effectively.</p>

<p><strong>PRO TIP âœ¨ğŸ“–ğŸ‘©ğŸ»â€ğŸ’»</strong> Lastly, you are in no way restricted by the availability of public dataset repositories. Sky's the limit. Grab a copy of <a href="https://twitter.com/DataForML">@DataForML</a> today and learn how to create your very own quality dataset using <a href="https://twitter.com/hashtag/Python">#Python</a> and other <a href="https://twitter.com/hashtag/OpenSource">#OpenSource</a> tools!</p>

</br>

<center>
 <iframe type="text/html" width="336" height="550" frameborder="0" allowfullscreen style="max-width:100%" src="https://read.amazon.com/kp/card?asin=B08RN47C5T&preview=inline&linkCode=kpe&ref_=cm_sw_r_kb_dp_TQ172033Z0X777T8FD8R&tag=mobile0a1329f-20" ></iframe>
</center>

</br>
<hr>
